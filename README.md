# âœ¨ LSTM Next Word Prediction â€” Neural Language Model

<p align="center">
  <img src="https://img.shields.io/badge/Deep%20Learning-LSTM-blue?style=for-the-badge">
  <img src="https://img.shields.io/badge/NLP-Sequence%20Modeling-green?style=for-the-badge">
  <img src="https://img.shields.io/badge/Streamlit-Interactive%20App-red?style=for-the-badge">
  <img src="https://img.shields.io/badge/Python-AI%20Project-yellow?style=for-the-badge">
</p>

<p align="center">
  ğŸš€ A Deep Learning NLP project that predicts the next word in a sentence using an LSTM-based neural language model.
</p>

---

## ğŸ“Œ Overview

This project demonstrates **Neural Language Modeling** using Long Short-Term Memory (LSTM) networks.
The model learns sequential text patterns and predicts the most probable next word given an input phrase.

The application includes:

* ğŸ§  Deep Learning LSTM architecture
* ğŸ“š NLP tokenization & sequence preprocessing
* âš¡ Real-time predictions using Streamlit
* ğŸ” Practical example of sequence modeling for intelligent text generation

---

## âœ¨ Features

* ğŸ”® Next-word prediction using trained LSTM
* ğŸ§© Tokenizer-based text preprocessing
* ğŸ“Š Early stopping training strategy
* âš¡ Fast interactive web interface
* ğŸ§  Demonstrates practical NLP + Deep Learning skills

---

## âš™ï¸ Tech Stack

| Technology         | Usage                   |
| ------------------ | ----------------------- |
| Python             | Core Programming        |
| TensorFlow / Keras | Deep Learning Model     |
| LSTM               | Sequence Prediction     |
| NLP Tokenizer      | Text Processing         |
| Streamlit          | Interactive UI          |
| Pickle             | Tokenizer Serialization |

---

## ğŸ“‚ Project Structure

```
LSTM RNN/
â”‚
â”œâ”€â”€ app.py                          # Streamlit application
â”œâ”€â”€ experiments.ipynb               # Training experiments
â”œâ”€â”€ hamlet.txt                      # Training dataset
â”œâ”€â”€ next_word_lstm.h5               # Trained LSTM model
â”œâ”€â”€ tokenizer.pickle                # Saved tokenizer
â”œâ”€â”€ requirements.txt                # Dependencies
```

---

## ğŸ§  Model Architecture

* Embedding & tokenization
* LSTM layers for sequence learning
* Dense output layer for word prediction
* Early stopping for optimized training

The model learns contextual patterns and generates intelligent text continuations.

---

## ğŸš€ Getting Started

### 1ï¸âƒ£ Clone Repository

```bash
git clone https://github.com/your-username/LSTM-Next-Word-Predictor.git
cd LSTM-Next-Word-Predictor
```

### 2ï¸âƒ£ Install Requirements

```bash
pip install -r requirements.txt
```

### 3ï¸âƒ£ Run Streamlit App

```bash
streamlit run app.py
```

---

## ğŸ–¥ï¸ How It Works

1. User enters a text sequence.
2. Text is tokenized and padded.
3. LSTM predicts probability distribution over vocabulary.
4. Highest probability word is returned as next prediction.

---

## ğŸ§ª Example

**Input:**

```
To be or not to
```

**Prediction:**

```
be
```

---

## ğŸ“Œ Future Improvements

* Transformer-based language models
* Top-K word prediction
* Temperature-based text generation
* Full sentence auto-completion
* Deployment on cloud platforms

---

## ğŸ‘¨â€ğŸ’» Author

**Vashishtha Verma**

* ğŸ¤– Machine Learning & Generative AI
* ğŸ§  Agentic AI Systems
* ğŸ’» Software Engineering & DSA

